{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y45yhDufI5Uo"
      },
      "source": [
        "# **RNN**\n",
        "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55i8db8uI5U3"
      },
      "source": [
        "IMDB sentiment classification task\n",
        "\n",
        "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. IMDB provided a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided.\n",
        "\n",
        "You can download the dataset from http://ai.stanford.edu/~amaas/data/sentiment/  or you can directly use \n",
        "\" from keras.datasets import imdb \" to import the dataset.\n",
        "\n",
        "Few points to be noted:\n",
        "Modules like SimpleRNN, LSTM, Activation layers, Dense layers, Dropout can be directly used from keras\n",
        "For preprocessing, you can use required "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SozhvLNkI5U6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f1bc51-b370-49f6-f291-25771b3a513c"
      },
      "source": [
        "#load the imdb dataset \n",
        "from keras.datasets import imdb\n",
        "\n",
        "vocabulary_size = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
        "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 25000 training samples, 25000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrilwfurI5VA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99da4c5d-675a-40e5-9b03-12c671d812d1"
      },
      "source": [
        "#the review is stored as a sequence of integers. \n",
        "# These are word IDs that have been pre-assigned to individual words, and the label is an integer\n",
        "\n",
        "print('---review---')\n",
        "print(X_train[0])\n",
        "print('---label---')\n",
        "print(y_train[0])\n",
        "\n",
        "# to get the actual review\n",
        "word2id = imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "print('---review with words---')\n",
        "print([id2word.get(i, ' ') for i in X_train[0]])\n",
        "print('---label---')\n",
        "print(y_train[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---review---\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "---label---\n",
            "1\n",
            "---review with words---\n",
            "['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'and', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'and', 'for', 'anyone', 'of', 'and', 'br', \"show's\", 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'odd', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'boat', 'she', 'he', 'should', 'is', 'thought', 'and', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'and', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'and', 'film', 'want', 'an']\n",
            "---label---\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6upWxEWI5VC"
      },
      "source": [
        "#pad sequences (write your code here)\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "tk = Tokenizer(lower=False) \n",
        "tk.fit_on_texts(X_train)\n",
        "X_train = tk.texts_to_sequences(X_train)\n",
        "X_test = tk.texts_to_sequences(X_test)\n",
        "\n",
        "tlens = [len(samp) for samp in X_train]\n",
        "maxLength = int(np.ceil(np.mean(tlens)))\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=maxLength, padding='post', truncating='post')\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=maxLength, padding='post', truncating='post')\n",
        "\n",
        "total_words = len(tk.word_index) + 1  \n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RcCOpeNI5VF"
      },
      "source": [
        "#design a RNN model (write your code)\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN\n",
        "\n",
        "dim = 32\n",
        "out = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, dim, input_length = maxLength))\n",
        "model.add(SimpleRNN(out))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQcvXNEhWSes"
      },
      "source": [
        "Adam Optimizer:<br>\n",
        "Adam optimizer involves a combination of two gradient descent methods: Momentum: This algorithm helps to speed up the gradient descent algorithm by taking into consideration the 'exponentially weighted average' of the gradients. So using averages makes the algorithm converge towards the minima in a faster rate<br>\n",
        "Binary Cross Entropy as Loss:<br>\n",
        "Given problem is binary classification so i choose binary cross entropy, binary cross entropy compares each predicted probabilities from dense layer to actual output and then calculates score by penalizing probability based on the distance of expected value. i.e how close from actual value. So this is best in this case.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InQ2TED3I5VI"
      },
      "source": [
        "#train and evaluate your model\n",
        "#choose your loss function and optimizer and mention the reason to choose that particular loss function and optimizer\n",
        "# use accuracy as the evaluation metric\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6A9Q0xmI5VJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b2b2d0-0c1c-4050-9843-4cdb72ceab0b"
      },
      "source": [
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "\n",
        "model.fit(X_train, y_train, batch_size = batch_size, epochs = num_epochs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 48s 60ms/step - loss: 0.6954 - accuracy: 0.5036\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.6929 - accuracy: 0.5209\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.6811 - accuracy: 0.5411\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.6671 - accuracy: 0.5616\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.6720 - accuracy: 0.5552\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.6530 - accuracy: 0.5710\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.6390 - accuracy: 0.5829\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.6092 - accuracy: 0.6006\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5826 - accuracy: 0.6150\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 47s 60ms/step - loss: 0.5610 - accuracy: 0.6288\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f87e515ed90>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTXG__EmI5VM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb98e7a-e2db-46a6-9863-d3e94d6dbdc9"
      },
      "source": [
        "#evaluate the model using model.evaluate()\n",
        "model.evaluate(X_test, y_test, verbose=0)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8340334296226501, 0.5077999830245972]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1uSo8DgI5VN"
      },
      "source": [
        "# **LSTM**\n",
        "\n",
        "Instead of using a RNN, now try using a LSTM model and compare both of them. Which of those performed better and why ?\n",
        "Ans: LSTM is better as it tackles the vanishing gradience problem and short memory in RNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk4rLYHwI5VP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9277c3e8-f1c2-4b8d-8da3-a9940666e1ad"
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN\n",
        "\n",
        "dim = 32\n",
        "out = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, dim, input_length = maxLength))\n",
        "model.add(LSTM(out))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "model.fit(X_train, y_train, batch_size = batch_size, epochs = num_epochs)\n",
        "model.evaluate(X_test, y_test, verbose=0)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 115s 145ms/step - loss: 0.6917 - accuracy: 0.5222\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 113s 144ms/step - loss: 0.6585 - accuracy: 0.5997\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 113s 144ms/step - loss: 0.6514 - accuracy: 0.6082\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 113s 145ms/step - loss: 0.6429 - accuracy: 0.6228\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 113s 144ms/step - loss: 0.6439 - accuracy: 0.6195\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 113s 144ms/step - loss: 0.6515 - accuracy: 0.5879\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 113s 144ms/step - loss: 0.6562 - accuracy: 0.5776\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 113s 144ms/step - loss: 0.6436 - accuracy: 0.5884\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 113s 145ms/step - loss: 0.6099 - accuracy: 0.6420\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 113s 145ms/step - loss: 0.5153 - accuracy: 0.7643\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5828858017921448, 0.7424799799919128]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBtRY9jmI5VQ"
      },
      "source": [
        "Perform Error analysis and explain using few examples.\n",
        "\n",
        "LSTMS are performing better than RNN whis is because<br>\n",
        "1)Because of the long texts, RNN is forgetting the previous context and continuing to evaluate the end, but LSTM has memory because of the gates<br>\n",
        "2)Key words are in the start of the samples and RNN does not have information about them.<br>\n",
        "For example: <br>\n",
        "1)the as you with brilliant of guy it used victor worst that it keep in long put this of now young of tried to answer instead he entire been its how korean this you<br>\n",
        "2)grade about hated it for br so ten remain by in of songs are of and and is morality it's her or know would care i i br screen that obvious plot actors new would with paris not have attempt lead or of too would local\n"
      ]
    }
  ]
}